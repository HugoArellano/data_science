{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2021-06-03T13:05:06.533020-05:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.7.6\n",
      "IPython version      : 7.12.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(datos.data, columns=datos.feature_names)\n",
    "boston[\"objetivo\"] = datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a evaluar los modelos que ya conocemos y compararlos con los distintos algoritmos de ensablado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_cv(estimador, X, y):\n",
    "    preds = estimador.predict(X)\n",
    "    return rmse(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': <function ndarray.mean>,\n",
       " 'elasticnet': 5.261057069533588,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969335}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "estimador_arbol = DecisionTreeRegressor()\n",
    "\n",
    "error_cv = cross_val_score(estimador_arbol, X = boston[datos.feature_names],\n",
    "                          y=boston[\"objetivo\"],\n",
    "                          scoring=rmse_cv, cv=10).mean\n",
    "\n",
    "resultados[\"arbol\"]=error_cv\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "\n",
    "estimador_elnet = ElasticNet()\n",
    "\n",
    "resultados[\"elasticnet\"] = cross_val_score(estimador_elnet, X=boston[datos.feature_names],y=boston[\"objetivo\"],\n",
    "                                           scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "estimador_lasso = Lasso()\n",
    "estimador_ridge = Ridge()\n",
    "\n",
    "resultados[\"lasso\"] = cross_val_score(estimador_lasso, X=boston[datos.feature_names], y=boston[\"objetivo\"],\n",
    "                                     scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"ridge\"] = cross_val_score(estimador_ridge, X=boston[datos.feature_names], y=boston[\"objetivo\"],\n",
    "                                     scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_cv(estimador, X, y):\n",
    "    preds = estimador.predict(X)\n",
    "    return rmse(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.930799537642789,\n",
       " 'elasticnet': 5.261057069533588,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969335}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "estimador_arbol = DecisionTreeRegressor()\n",
    "\n",
    "error_cv = cross_val_score(estimador_arbol, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"arbol\"] = error_cv\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "\n",
    "estimador_elnet = ElasticNet()\n",
    "\n",
    "resultados[\"elasticnet\"] = cross_val_score(estimador_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "\n",
    "estimador_lasso = Lasso()\n",
    "estimador_ridge = Ridge()\n",
    "\n",
    "\n",
    "resultados[\"lasso\"] = cross_val_score(estimador_lasso, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"ridge\"] = cross_val_score(estimador_ridge, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de Bagging (Bootstrap aggregating) funcionan entrenando varios estimadores base y cambiando los datos de entrenamiento para cada uno. En sklearn los algoritmos de ensmblado de modelos se encuentran en el submodulo sklearn.ensemble. En cuanto a Bagging, sklearn tiene una version para problemas de clasificació y otra para problemas de regresion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.ensemble import BaggingRegressor, BaggingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bagging regressor.\n",
      "\n",
      "    A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "    regressors each on random subsets of the original dataset and then\n",
      "    aggregate their individual predictions (either by voting or by averaging)\n",
      "    to form a final prediction. Such a meta-estimator can typically be used as\n",
      "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "    tree), by introducing randomization into its construction procedure and\n",
      "    then making an ensemble out of it.\n",
      "\n",
      "    This algorithm encompasses several works from the literature. When random\n",
      "    subsets of the dataset are drawn as random subsets of the samples, then\n",
      "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "    of the dataset are drawn as random subsets of the features, then the method\n",
      "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "    on subsets of both samples and features, then the method is known as\n",
      "    Random Patches [4]_.\n",
      "\n",
      "    Read more in the :ref:`User Guide <bagging>`.\n",
      "\n",
      "    .. versionadded:: 0.15\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object or None, optional (default=None)\n",
      "        The base estimator to fit on random subsets of the dataset.\n",
      "        If None, then the base estimator is a decision tree.\n",
      "\n",
      "    n_estimators : int, optional (default=10)\n",
      "        The number of base estimators in the ensemble.\n",
      "\n",
      "    max_samples : int or float, optional (default=1.0)\n",
      "        The number of samples to draw from X to train each base estimator.\n",
      "\n",
      "        - If int, then draw `max_samples` samples.\n",
      "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "    max_features : int or float, optional (default=1.0)\n",
      "        The number of features to draw from X to train each base estimator.\n",
      "\n",
      "        - If int, then draw `max_features` features.\n",
      "        - If float, then draw `max_features * X.shape[1]` features.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether samples are drawn with replacement. If False, sampling\n",
      "        without replacement is performed.\n",
      "\n",
      "    bootstrap_features : boolean, optional (default=False)\n",
      "        Whether features are drawn with replacement.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to True, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit\n",
      "        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    n_jobs : int or None, optional (default=None)\n",
      "        The number of jobs to run in parallel for both :meth:`fit` and\n",
      "        :meth:`predict`. ``None`` means 1 unless in a\n",
      "        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "        processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    base_estimator_ : estimator\n",
      "        The base estimator from which the ensemble is grown.\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when :meth:`fit` is performed.\n",
      "\n",
      "    estimators_ : list of estimators\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimators_samples_ : list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator. Each subset is defined by an array of the indices selected.\n",
      "\n",
      "    estimators_features_ : list of arrays\n",
      "        The subset of drawn features for each base estimator.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "        This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "    oob_prediction_ : ndarray of shape (n_samples,)\n",
      "        Prediction computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_prediction_` might contain NaN. This attribute exists only\n",
      "        when ``oob_score`` is True.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.svm import SVR\n",
      "    >>> from sklearn.ensemble import BaggingRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "    >>> X, y = make_regression(n_samples=100, n_features=4,\n",
      "    ...                        n_informative=2, n_targets=1,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = BaggingRegressor(base_estimator=SVR(),\n",
      "    ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      "    >>> regr.predict([[0, 0, 0, 0]])\n",
      "    array([-2.8720...])\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "\n",
      "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "           1996.\n",
      "\n",
      "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "           1998.\n",
      "\n",
      "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BaggingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bagging regressor.\n",
      "\n",
      "    A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "    regressors each on random subsets of the original dataset and then\n",
      "    aggregate their individual predictions (either by voting or by averaging)\n",
      "    to form a final prediction. Such a meta-estimator can typically be used as\n",
      "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "    tree), by introducing randomization into its construction procedure and\n",
      "    then making an ensemble out of it.\n",
      "\n",
      "    This algorithm encompasses several works from the literature. When random\n",
      "    subsets of the dataset are drawn as random subsets of the samples, then\n",
      "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "    of the dataset are drawn as random subsets of the features, then the method\n",
      "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "    on subsets of both samples and features, then the method is known as\n",
      "    Random Patches [4]_.\n",
      "\n",
      "    Read more in the :ref:`User Guide <bagging>`.\n",
      "\n",
      "    .. versionadded:: 0.15\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object or None, optional (default=None)\n",
      "        The base estimator to fit on random subsets of the dataset.\n",
      "        If None, then the base estimator is a decision tree.\n",
      "\n",
      "    n_estimators : int, optional (default=10)\n",
      "        The number of base estimators in the ensemble.\n",
      "\n",
      "    max_samples : int or float, optional (default=1.0)\n",
      "        The number of samples to draw from X to train each base estimator.\n",
      "\n",
      "        - If int, then draw `max_samples` samples.\n",
      "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "    max_features : int or float, optional (default=1.0)\n",
      "        The number of features to draw from X to train each base estimator.\n",
      "\n",
      "        - If int, then draw `max_features` features.\n",
      "        - If float, then draw `max_features * X.shape[1]` features.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether samples are drawn with replacement. If False, sampling\n",
      "        without replacement is performed.\n",
      "\n",
      "    bootstrap_features : boolean, optional (default=False)\n",
      "        Whether features are drawn with replacement.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to True, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit\n",
      "        a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    n_jobs : int or None, optional (default=None)\n",
      "        The number of jobs to run in parallel for both :meth:`fit` and\n",
      "        :meth:`predict`. ``None`` means 1 unless in a\n",
      "        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "        processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    base_estimator_ : estimator\n",
      "        The base estimator from which the ensemble is grown.\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when :meth:`fit` is performed.\n",
      "\n",
      "    estimators_ : list of estimators\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimators_samples_ : list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator. Each subset is defined by an array of the indices selected.\n",
      "\n",
      "    estimators_features_ : list of arrays\n",
      "        The subset of drawn features for each base estimator.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "        This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "    oob_prediction_ : ndarray of shape (n_samples,)\n",
      "        Prediction computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_prediction_` might contain NaN. This attribute exists only\n",
      "        when ``oob_score`` is True.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.svm import SVR\n",
      "    >>> from sklearn.ensemble import BaggingRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "    >>> X, y = make_regression(n_samples=100, n_features=4,\n",
      "    ...                        n_informative=2, n_targets=1,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = BaggingRegressor(base_estimator=SVR(),\n",
      "    ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      "    >>> regr.predict([[0, 0, 0, 0]])\n",
      "    array([-2.8720...])\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "\n",
      "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "           1996.\n",
      "\n",
      "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "           1998.\n",
      "\n",
      "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BaggingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaggingRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.464606082289898"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_10 = BaggingRegressor(n_estimators=10)\n",
    "error_cv = cross_val_score(estimador_bagging_10, X=boston[datos.feature_names], y=boston[\"objetivo\"],\n",
    "                          scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingRegressor utiliza arboles de decisión como estimador base por defecto, sin embargo, podemos utilizar uno distinto mediante el parametro base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ElasticNet())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"],\n",
    "                          scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_elnet\"] = error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.255074676269464"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su momento vimos que existe un tipo de arbol de decision completamente aleatorio (Extremely Randomized Trees) que deciden la particion en cada nodo al azar. Vemos que al agrupar muchos de estos estimadores que son débiles (aun que mejores que tirar una moneda al azar, ya que un árbol de decision aleatorio aun asi aprende a separar los elementos), la varianza general se reduce ya que la que aporta un arbol se complementa con la del de al lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.933019635315263"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeRegressor\n",
    "\n",
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ExtraTreeRegressor())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"],\n",
    "                          scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_extra_arbol\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de boosting intentan mejorar los estimadores base asignando pesos en funcion de su funcionamiento individual. El algoritmo clásico de boosting es AdaBoost, que se encuentra en sklearn como AdaBoostRegressor y AdaBoostClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AdaBoost regressor.\n",
      "\n",
      "    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "    regressor on the original dataset and then fits additional copies of the\n",
      "    regressor on the same dataset but where the weights of instances are\n",
      "    adjusted according to the error of the current prediction. As such,\n",
      "    subsequent regressors focus more on difficult cases.\n",
      "\n",
      "    This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "\n",
      "    Read more in the :ref:`User Guide <adaboost>`.\n",
      "\n",
      "    .. versionadded:: 0.14\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object, optional (default=None)\n",
      "        The base estimator from which the boosted ensemble is built.\n",
      "        If ``None``, then the base estimator is\n",
      "        ``DecisionTreeRegressor(max_depth=3)``.\n",
      "\n",
      "    n_estimators : integer, optional (default=50)\n",
      "        The maximum number of estimators at which boosting is terminated.\n",
      "        In case of perfect fit, the learning procedure is stopped early.\n",
      "\n",
      "    learning_rate : float, optional (default=1.)\n",
      "        Learning rate shrinks the contribution of each regressor by\n",
      "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "        ``n_estimators``.\n",
      "\n",
      "    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      "        The loss function to use when updating the weights after each\n",
      "        boosting iteration.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    base_estimator_ : estimator\n",
      "        The base estimator from which the ensemble is grown.\n",
      "\n",
      "    estimators_ : list of classifiers\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimator_weights_ : array of floats\n",
      "        Weights for each estimator in the boosted ensemble.\n",
      "\n",
      "    estimator_errors_ : array of floats\n",
      "        Regression error for each estimator in the boosted ensemble.\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The feature importances if supported by the ``base_estimator``.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import AdaBoostRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
      "    >>> regr.fit(X, y)\n",
      "    AdaBoostRegressor(n_estimators=100, random_state=0)\n",
      "    >>> regr.feature_importances_\n",
      "    array([0.2788..., 0.7109..., 0.0065..., 0.0036...])\n",
      "    >>> regr.predict([[0, 0, 0, 0]])\n",
      "    array([4.7972...])\n",
      "    >>> regr.score(X, y)\n",
      "    0.9771...\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    AdaBoostClassifier, GradientBoostingRegressor,\n",
      "    sklearn.tree.DecisionTreeRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "           on-Line Learning and an Application to Boosting\", 1995.\n",
      "\n",
      "    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(AdaBoostRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.442665762621013"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_adaboost = AdaBoostRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_adaboost, X=boston[datos.feature_names], y=boston[\"objetivo\"],\n",
    "                          scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"adaboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro algoritmo de Boosting es Gradient Boosting que a cada iteración usa el algoritmo de Descenso de Gradiente (que veremos en el futuro) para a cada iteración , entrenar un estimador nuevo que minimiza la función de error (loss function) del modelo.\n",
    "\n",
    "Scikit-learn implementa el algoritmo de (Gradient Boosted Regression Trees), que usa árboles de decisión como estimadores base, en GradientBoostingRegressor y GradientBoostingClassifier\n",
    "\n",
    "Gradient Boosting puede usar cualquier funcion de error siempre que sea diferenciable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting for regression.\n",
      "\n",
      "    GB builds an additive model in a forward stage-wise fashion;\n",
      "    it allows for the optimization of arbitrary differentiable loss functions.\n",
      "    In each stage a regression tree is fit on the negative gradient of the\n",
      "    given loss function.\n",
      "\n",
      "    Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      "        loss function to be optimized. 'ls' refers to least squares\n",
      "        regression. 'lad' (least absolute deviation) is a highly robust\n",
      "        loss function solely based on order information of the input\n",
      "        variables. 'huber' is a combination of the two. 'quantile'\n",
      "        allows quantile regression (use `alpha` to specify the quantile).\n",
      "\n",
      "    learning_rate : float, optional (default=0.1)\n",
      "        learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "        There is a trade-off between learning_rate and n_estimators.\n",
      "\n",
      "    n_estimators : int (default=100)\n",
      "        The number of boosting stages to perform. Gradient boosting\n",
      "        is fairly robust to over-fitting so a large number usually\n",
      "        results in better performance.\n",
      "\n",
      "    subsample : float, optional (default=1.0)\n",
      "        The fraction of samples to be used for fitting the individual base\n",
      "        learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "        Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "        Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "    criterion : string, optional (default=\"friedman_mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"friedman_mse\" for the mean squared error with improvement\n",
      "        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "        the mean absolute error. The default value of \"friedman_mse\" is\n",
      "        generally the best as it can provide a better approximation in\n",
      "        some cases.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_depth : integer, optional (default=3)\n",
      "        maximum depth of the individual regression estimators. The maximum\n",
      "        depth limits the number of nodes in the tree. Tune this parameter\n",
      "        for best performance; the best value depends on the interaction\n",
      "        of the input variables.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    min_impurity_split : float, (default=1e-7)\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19. The default value of\n",
      "           ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      "           will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    init : estimator or 'zero', optional (default=None)\n",
      "        An estimator object that is used to compute the initial predictions.\n",
      "        ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
      "        initial raw predictions are set to zero. By default a\n",
      "        ``DummyEstimator`` is used, predicting either the average target value\n",
      "        (for loss='ls'), or a quantile for the other losses.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=None)\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Choosing `max_features < n_features` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    alpha : float (default=0.9)\n",
      "        The alpha-quantile of the huber loss function and the quantile\n",
      "        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "\n",
      "    verbose : int, default: 0\n",
      "        Enable verbose output. If 1 then it prints progress and performance\n",
      "        once in a while (the more trees the lower the frequency). If greater\n",
      "        than 1 then it prints progress and performance for every tree.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    warm_start : bool, default: False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just erase the\n",
      "        previous solution. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    presort : deprecated, default='deprecated'\n",
      "        This parameter is deprecated and will be removed in v0.24.\n",
      "\n",
      "        .. deprecated :: 0.22\n",
      "\n",
      "    validation_fraction : float, optional, default 0.1\n",
      "        The proportion of training data to set aside as validation set for\n",
      "        early stopping. Must be between 0 and 1.\n",
      "        Only used if ``n_iter_no_change`` is set to an integer.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "    n_iter_no_change : int, default None\n",
      "        ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "        to terminate training when validation score is not improving. By\n",
      "        default it is set to None to disable early stopping. If set to a\n",
      "        number, it will set aside ``validation_fraction`` size of the training\n",
      "        data as validation and terminate training when validation score is not\n",
      "        improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "        iterations.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "    tol : float, optional, default 1e-4\n",
      "        Tolerance for the early stopping. When the loss is not improving\n",
      "        by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "        number), the training stops.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "    ccp_alpha : non-negative float, optional (default=0.0)\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    feature_importances_ : array, shape (n_features,)\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    oob_improvement_ : array, shape (n_estimators,)\n",
      "        The improvement in loss (= deviance) on the out-of-bag samples\n",
      "        relative to the previous iteration.\n",
      "        ``oob_improvement_[0]`` is the improvement in\n",
      "        loss of the first stage over the ``init`` estimator.\n",
      "        Only available if ``subsample < 1.0``\n",
      "\n",
      "    train_score_ : array, shape (n_estimators,)\n",
      "        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "        model at iteration ``i`` on the in-bag sample.\n",
      "        If ``subsample == 1`` this is the deviance on the training data.\n",
      "\n",
      "    loss_ : LossFunction\n",
      "        The concrete ``LossFunction`` object.\n",
      "\n",
      "    init_ : estimator\n",
      "        The estimator that provides the initial predictions.\n",
      "        Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "\n",
      "    estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data and\n",
      "    ``max_features=n_features``, if the improvement of the criterion is\n",
      "    identical for several splits enumerated during the search of the best\n",
      "    split. To obtain a deterministic behaviour during fitting,\n",
      "    ``random_state`` has to be fixed.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    sklearn.ensemble.HistGradientBoostingRegressor,\n",
      "    sklearn.tree.DecisionTreeRegressor, RandomForestRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "\n",
      "    J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "\n",
      "    T. Hastie, R. Tibshirani and J. Friedman.\n",
      "    Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(GradientBoostingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8877327620537825"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_gradientboost = GradientBoostingRegressor(n_estimators=100, \n",
    "                                                   loss=\"huber\")\n",
    "\n",
    "error_cv = cross_val_score(estimador_gradientboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                          scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"gradientboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cualquier estimador basado en árboles, GradientBoostRegressor nos permite ver la importancia de las variables en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimador_gradientboost.fit(boston[datos.feature_names], boston.objetivo)\n",
    "\n",
    "importancia_variables = estimador_gradientboost.feature_importances_\n",
    "importancia_variables = 100.0 * (importancia_variables / importancia_variables.max())\n",
    "sorted_idx = np.argsort(importancia_variables)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.barh(pos, importancia_variables[sorted_idx], align='center')\n",
    "plt.yticks(pos, datos.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bosques Aleatorios (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de Bosques Aleatorios funciona mediante la creación de árboles de decision entrenados en un subgrupo aleatorio de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.192368736484096"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_randomforest = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_randomforest, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"randomforest_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.930799537642789,\n",
       " 'elasticnet': 5.261057069533588,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969335,\n",
       " 'bagging_arbol_100': 4.464606082289898,\n",
       " 'bagging_elnet': 5.255074676269464,\n",
       " 'bagging_extra_arbol': 3.933019635315263,\n",
       " 'adaboost_100': 4.442665762621013,\n",
       " 'gradientboost_100': 3.8877327620537825,\n",
       " 'randomforest_100': 4.192368736484096}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "XGBoost (eXtreme Gradient Boosting) es un algoritmo de boosting relativamente nuevo que tiene bastante acogida. Es una implementación de Gradient Boosted Trees pero enfocado a datasets grandes.\n",
    "\n",
    "Al ser muy nuevo (el proyecto se creó en 2014 y el paper se publicó en 2016, éste es el paper) no está implementado en scikit-learn, sin embargo existe en el paquete xgboost, que proporciona estimadores en base a dicho algoritmo que son compatibles con sklearn.\n",
    "\n",
    "Podemos instalar xgboost de conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\carlo\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0          11 KB  conda-forge\n",
      "    libxgboost-1.4.0           |       h0e60522_0         2.2 MB  conda-forge\n",
      "    py-xgboost-1.4.0           |   py37h03978a9_0         140 KB  conda-forge\n",
      "    xgboost-1.4.0              |   py37h03978a9_0          11 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/win-64::_py-xgboost-mutex-2.0-cpu_0\n",
      "  libxgboost         conda-forge/win-64::libxgboost-1.4.0-h0e60522_0\n",
      "  py-xgboost         conda-forge/win-64::py-xgboost-1.4.0-py37h03978a9_0\n",
      "  xgboost            conda-forge/win-64::xgboost-1.4.0-py37h03978a9_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "py-xgboost-1.4.0     | 140 KB    |            |   0% \n",
      "py-xgboost-1.4.0     | 140 KB    | #1         |  11% \n",
      "py-xgboost-1.4.0     | 140 KB    | #########1 |  91% \n",
      "py-xgboost-1.4.0     | 140 KB    | ########## | 100% \n",
      "\n",
      "xgboost-1.4.0        | 11 KB     |            |   0% \n",
      "xgboost-1.4.0        | 11 KB     | ########## | 100% \n",
      "xgboost-1.4.0        | 11 KB     | ########## | 100% \n",
      "\n",
      "_py-xgboost-mutex-2. | 11 KB     |            |   0% \n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \n",
      "_py-xgboost-mutex-2. | 11 KB     | ########## | 100% \n",
      "\n",
      "libxgboost-1.4.0     | 2.2 MB    |            |   0% \n",
      "libxgboost-1.4.0     | 2.2 MB    |            |   1% \n",
      "libxgboost-1.4.0     | 2.2 MB    | 7          |   7% \n",
      "libxgboost-1.4.0     | 2.2 MB    | #8         |  18% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ##6        |  26% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ###1       |  31% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ####6      |  46% \n",
      "libxgboost-1.4.0     | 2.2 MB    | #######2   |  72% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ########4  |  85% \n",
      "libxgboost-1.4.0     | 2.2 MB    | #########4 |  95% \n",
      "libxgboost-1.4.0     | 2.2 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    n_estimators : int\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth : int\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : float\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    verbosity : int\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "    objective : string or callable\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: string\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    tree_method: string\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter\n",
      "        is set to default, XGBoost will choose the most conservative option\n",
      "        available.  It's recommended to study this option from parameters\n",
      "        document.\n",
      "    n_jobs : int\n",
      "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "        balance the threads.  Creating thread contention will significantly slow down both\n",
      "        algorithms.\n",
      "    gamma : float\n",
      "        Minimum loss reduction required to make a further partition on a leaf\n",
      "        node of the tree.\n",
      "    min_child_weight : float\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : float\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : float\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : float\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : float\n",
      "        Subsample ratio of columns for each level.\n",
      "    colsample_bynode : float\n",
      "        Subsample ratio of columns for each split.\n",
      "    reg_alpha : float (xgb's alpha)\n",
      "        L1 regularization term on weights\n",
      "    reg_lambda : float (xgb's lambda)\n",
      "        L2 regularization term on weights\n",
      "    scale_pos_weight : float\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score:\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    random_state : int\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float, default np.nan\n",
      "        Value in the data which needs to be present as a missing value.\n",
      "    num_parallel_tree: int\n",
      "        Used for boosting random forest.\n",
      "    monotone_constraints : str\n",
      "        Constraint of variable monotonicity.  See tutorial for more\n",
      "        information.\n",
      "    interaction_constraints : str\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "        [2, 3, 4]], where each inner list is a group of indices of features\n",
      "        that are allowed to interact with each other.  See tutorial for more\n",
      "        information\n",
      "    importance_type: string, default \"gain\"\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "        either \"gain\", \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "    gpu_id :\n",
      "        Device ordinal.\n",
      "    validate_parameters :\n",
      "        Give warnings for unknown parameter.\n",
      "\n",
      "    \\*\\*kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "        parameters can be found here:\n",
      "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n",
      "        .. note::  Custom objective function\n",
      "\n",
      "            A custom objective function can be provided for the ``objective``\n",
      "            parameter. In this case, it should have the signature\n",
      "            ``objective(y_true, y_pred) -> grad, hess``:\n",
      "\n",
      "            y_true: array_like of shape [n_samples]\n",
      "                The target values\n",
      "            y_pred: array_like of shape [n_samples]\n",
      "                The predicted values\n",
      "\n",
      "            grad: array_like of shape [n_samples]\n",
      "                The value of the gradient for each sample point.\n",
      "            hess: array_like of shape [n_samples]\n",
      "                The value of the second derivative for each sample point\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(XGBRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n",
      "C:\\Users\\carlo\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.369623838543127"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_xgboost = XGBRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_xgboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"xgboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import plot_importance, to_graphviz\n",
    "estimador_xgboost.fit(boston[datos.feature_names], boston.objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20635d9ab88>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(estimador_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"1813pt\" height=\"1178pt\"\r\n",
       " viewBox=\"0.00 0.00 1813.10 1178.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1174)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1174 1809.1,-1174 1809.1,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"82.5427\" cy=\"-683\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"82.5427\" y=\"-679.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LSTAT&lt;4.6500001</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"351.328\" cy=\"-720\" rx=\"81.4863\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"351.328\" y=\"-716.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">CRIM&lt;3.45420504</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M152.594,-692.577C189.159,-697.648 234.205,-703.895 272.063,-709.146\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"271.643,-712.621 282.029,-710.528 272.604,-705.687 271.643,-712.621\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"217.585\" y=\"-708.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"351.328\" cy=\"-637\" rx=\"72.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"351.328\" y=\"-633.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DIS&lt;1.17165005</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M147.784,-671.923C188.094,-664.973 240.049,-656.015 281.246,-648.911\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"281.945,-652.343 291.205,-647.194 280.755,-645.444 281.945,-652.343\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"217.585\" y=\"-668.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"612.964\" cy=\"-847\" rx=\"72.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"612.964\" y=\"-843.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DIS&lt;5.97360039</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M385.739,-736.343C432.801,-759.363 518.587,-801.325 569.923,-826.436\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"568.57,-829.67 579.09,-830.92 571.645,-823.382 568.57,-829.67\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"485.071\" y=\"-801.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"612.964\" cy=\"-720\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"612.964\" y=\"-716.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.943039834</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M432.701,-720C462.673,-720 496.779,-720 527.139,-720\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"527.28,-723.5 537.28,-720 527.28,-716.5 527.28,-723.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"485.071\" y=\"-723.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node28\" class=\"node\"><title>5</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"612.964\" cy=\"-637\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"612.964\" y=\"-633.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.681344748</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;5 -->\r\n",
       "<g id=\"edge27\" class=\"edge\"><title>2&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M424.306,-637C456.132,-637 493.786,-637 526.982,-637\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"527.129,-640.5 537.129,-637 527.129,-633.5 527.129,-640.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"485.071\" y=\"-640.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node29\" class=\"node\"><title>6</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"612.964\" cy=\"-481\" rx=\"72.2875\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"612.964\" y=\"-477.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RM&lt;6.61100006</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge28\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M380.114,-620.305C427.354,-591.922 523.289,-534.28 575.731,-502.77\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"577.609,-505.725 584.378,-497.575 574.004,-499.725 577.609,-505.725\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"485.071\" y=\"-580.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"878.5\" cy=\"-1007\" rx=\"85.2851\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"878.5\" y=\"-1003.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">INDUS&lt;2.78500009</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;7 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M641.669,-863.82C689.39,-892.793 787.108,-952.12 840.559,-984.572\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"839.009,-987.725 849.373,-989.923 842.642,-981.742 839.009,-987.725\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"740.858\" y=\"-944.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>8</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"878.5\" cy=\"-847\" rx=\"63.8893\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"878.5\" y=\"-843.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">B&lt;386.214996</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;8 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>3&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M685.977,-847C722.752,-847 767.406,-847 804.344,-847\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"804.544,-850.5 814.544,-847 804.544,-843.5 804.544,-850.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"740.858\" y=\"-850.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>11</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-1061\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-1057.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.619758487</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;11 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M941.997,-1019.12C985.978,-1027.65 1044.82,-1039.06 1089.97,-1047.81\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1089.49,-1051.28 1099.97,-1049.75 1090.82,-1044.41 1089.49,-1051.28\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-1041.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>12</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-1007\" rx=\"63.8893\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-1003.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">B&lt;382.419983</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;12 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M963.81,-1007C1002.44,-1007 1047.79,-1007 1084.94,-1007\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1085.19,-1010.5 1095.19,-1007 1085.19,-1003.5 1085.19,-1010.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-1010.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>13</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-882\" rx=\"90.1842\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-878.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">CRIM&lt;0.0174349993</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;13 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>8&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M936.864,-854.21C975.862,-859.111 1028.15,-865.682 1072.09,-871.204\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1071.83,-874.7 1082.19,-872.474 1072.71,-867.755 1071.83,-874.7\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-870.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>14</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-811\" rx=\"77.1866\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-807.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">NOX&lt;0.44749999</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;14 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>8&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M936.864,-839.584C978.46,-834.207 1035.18,-826.876 1080.75,-820.984\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1081.42,-824.427 1090.89,-819.673 1080.52,-817.484 1081.42,-824.427\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-836.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>19</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-1098\" rx=\"85.5853\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-1094.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">CRIM&lt;0.179739997</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;19 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>12&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1203.67,-1019.97C1218.22,-1024.35 1234.49,-1029.32 1249.33,-1034 1296.64,-1048.94 1350.17,-1066.62 1389.05,-1079.62\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1388.02,-1082.96 1398.61,-1082.81 1390.24,-1076.32 1388.02,-1082.96\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-1064.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>20</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-1007\" rx=\"46.2923\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-1003.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RAD&lt;6.5</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;20 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>12&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1222.94,-1007C1272.52,-1007 1340.95,-1007 1388.42,-1007\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1388.51,-1010.5 1398.51,-1007 1388.51,-1003.5 1388.51,-1010.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-1010.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 33 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>33</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-1152\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-1148.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.146436885</text>\r\n",
       "</g>\r\n",
       "<!-- 19&#45;&gt;33 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>19&#45;&gt;33</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1507.93,-1110.19C1551.36,-1118.69 1609.31,-1130.03 1653.89,-1138.76\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1653.27,-1142.2 1663.76,-1140.69 1654.62,-1135.33 1653.27,-1142.2\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-1134.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 34 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>34</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-1098\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-1094.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.534953952</text>\r\n",
       "</g>\r\n",
       "<!-- 19&#45;&gt;34 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>19&#45;&gt;34</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1530.69,-1098C1564.41,-1098 1603.1,-1098 1636.79,-1098\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1637.08,-1101.5 1647.08,-1098 1637.08,-1094.5 1637.08,-1101.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-1101.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 35 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>35</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-1044\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-1040.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.267285854</text>\r\n",
       "</g>\r\n",
       "<!-- 20&#45;&gt;35 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>20&#45;&gt;35</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1488.47,-1012.74C1530.61,-1018.39 1595.85,-1027.14 1646.6,-1033.95\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1646.17,-1037.42 1656.55,-1035.28 1647.1,-1030.48 1646.17,-1037.42\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-1032.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 36 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>36</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-990\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-986.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.123410232</text>\r\n",
       "</g>\r\n",
       "<!-- 20&#45;&gt;36 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>20&#45;&gt;36</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1490.3,-1004.25C1529.81,-1001.82 1588.54,-998.196 1636.93,-995.214\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1637.33,-998.697 1647.09,-994.588 1636.89,-991.71 1637.33,-998.697\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-1003.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 21 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>21</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-936\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-932.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0953445509</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;21 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>13&#45;&gt;21</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1225.07,-894.389C1269.32,-902.813 1327.84,-913.952 1373.2,-922.586\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1372.77,-926.067 1383.25,-924.499 1374.08,-919.191 1372.77,-926.067\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-918.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 22 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>22</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-882\" rx=\"72.2875\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-878.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">AGE&lt;7.8499999</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;22 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>13&#45;&gt;22</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1249.35,-882C1285.42,-882 1326.76,-882 1362.01,-882\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1362.26,-885.5 1372.26,-882 1362.26,-878.5 1362.26,-885.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-885.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 23 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>23</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-811\" rx=\"46.2923\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-807.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RAD&lt;2.5</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;23 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>14&#45;&gt;23</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1236.38,-811C1284.24,-811 1344.82,-811 1388.12,-811\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1388.18,-814.5 1398.18,-811 1388.18,-807.5 1388.18,-814.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-814.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 24 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>24</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-720\" rx=\"90.1842\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-716.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">CRIM&lt;0.0314299986</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;24 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>14&#45;&gt;24</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1205.25,-796.48C1254.43,-780.702 1333.13,-755.459 1386.53,-738.327\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1387.75,-741.614 1396.2,-735.227 1385.61,-734.949 1387.75,-741.614\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-779.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 37 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>37</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-936\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-932.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0486082099</text>\r\n",
       "</g>\r\n",
       "<!-- 22&#45;&gt;37 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>22&#45;&gt;37</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1501.83,-892.993C1545.13,-901.468 1605.25,-913.236 1651.67,-922.321\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1651.16,-925.788 1661.65,-924.275 1652.51,-918.919 1651.16,-925.788\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-918.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 38 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>38</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-882\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-878.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0242429748</text>\r\n",
       "</g>\r\n",
       "<!-- 22&#45;&gt;38 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>22&#45;&gt;38</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1517.07,-882C1551.37,-882 1592.88,-882 1629.51,-882\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1629.71,-885.5 1639.71,-882 1629.71,-878.5 1629.71,-885.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-885.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 39 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>39</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-828\" rx=\"70.6878\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-824.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.09441223</text>\r\n",
       "</g>\r\n",
       "<!-- 23&#45;&gt;39 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>23&#45;&gt;39</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1490.3,-813.75C1531.6,-816.295 1593.92,-820.135 1643.46,-823.188\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1643.31,-826.685 1653.51,-823.807 1643.74,-819.698 1643.31,-826.685\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-824.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 40 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>40</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-774\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-770.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.329651266</text>\r\n",
       "</g>\r\n",
       "<!-- 23&#45;&gt;40 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>23&#45;&gt;40</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1488.47,-805.259C1530.61,-799.608 1595.85,-790.858 1646.6,-784.052\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1647.1,-787.516 1656.55,-782.718 1646.17,-780.578 1647.1,-787.516\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-798.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 41 -->\r\n",
       "<g id=\"node26\" class=\"node\"><title>41</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-720\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-716.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0404276848</text>\r\n",
       "</g>\r\n",
       "<!-- 24&#45;&gt;41 -->\r\n",
       "<g id=\"edge25\" class=\"edge\"><title>24&#45;&gt;41</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1535.24,-720C1565.49,-720 1599.29,-720 1629.72,-720\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1629.9,-723.5 1639.9,-720 1629.9,-716.5 1629.9,-723.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-723.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 42 -->\r\n",
       "<g id=\"node27\" class=\"node\"><title>42</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-666\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-662.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0570339225</text>\r\n",
       "</g>\r\n",
       "<!-- 24&#45;&gt;42 -->\r\n",
       "<g id=\"edge26\" class=\"edge\"><title>24&#45;&gt;42</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1509.66,-707.476C1552.19,-699.151 1608.03,-688.22 1651.72,-679.67\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1652.55,-683.073 1661.69,-677.717 1651.2,-676.203 1652.55,-683.073\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-700.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node30\" class=\"node\"><title>9</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"878.5\" cy=\"-481\" rx=\"54.6905\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"878.5\" y=\"-477.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">TAX&lt;222.5</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;9 -->\r\n",
       "<g id=\"edge29\" class=\"edge\"><title>6&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M685.281,-481C725.301,-481 774.82,-481 813.574,-481\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"813.69,-484.5 823.69,-481 813.69,-477.5 813.69,-484.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"740.858\" y=\"-484.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node31\" class=\"node\"><title>10</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"878.5\" cy=\"-207\" rx=\"71.4873\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"878.5\" y=\"-203.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">PTRATIO&lt;16.5</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;10 -->\r\n",
       "<g id=\"edge30\" class=\"edge\"><title>6&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M630.85,-463.443C676.204,-416.289 800.789,-286.757 853.472,-231.982\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"856.004,-234.399 860.413,-224.765 850.959,-229.546 856.004,-234.399\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"740.858\" y=\"-387.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node32\" class=\"node\"><title>15</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-504\" rx=\"59.5901\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-500.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">B&lt;395.51001</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;15 -->\r\n",
       "<g id=\"edge31\" class=\"edge\"><title>9&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M931.554,-485.299C976.528,-489.014 1041.81,-494.405 1090.73,-498.446\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1090.66,-501.951 1100.91,-499.286 1091.23,-494.975 1090.66,-501.951\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-497.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node33\" class=\"node\"><title>16</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-450\" rx=\"46.2923\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RAD&lt;1.5</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;16 -->\r\n",
       "<g id=\"edge32\" class=\"edge\"><title>9&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M930.578,-475.314C979.791,-469.836 1053.96,-461.579 1104.1,-455.998\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1104.55,-459.47 1114.1,-454.885 1103.78,-452.513 1104.55,-459.47\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-472.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 17 -->\r\n",
       "<g id=\"node44\" class=\"node\"><title>17</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-207\" rx=\"72.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-203.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DIS&lt;7.56839991</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;17 -->\r\n",
       "<g id=\"edge43\" class=\"edge\"><title>10&#45;&gt;17</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M950.133,-207C988.378,-207 1035.83,-207 1075.73,-207\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1075.97,-210.5 1085.97,-207 1075.97,-203.5 1075.97,-210.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-210.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 18 -->\r\n",
       "<g id=\"node45\" class=\"node\"><title>18</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1158.98\" cy=\"-72\" rx=\"76.8869\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1158.98\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">AGE&lt;99.0500031</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;18 -->\r\n",
       "<g id=\"edge44\" class=\"edge\"><title>10&#45;&gt;18</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M912.265,-191.117C962.69,-166.672 1059.33,-119.826 1115.21,-92.7373\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1117.01,-95.7541 1124.48,-88.2425 1113.95,-89.4552 1117.01,-95.7541\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1016.14\" y=\"-158.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 25 -->\r\n",
       "<g id=\"node34\" class=\"node\"><title>25</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-558\" rx=\"63.8893\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-554.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">B&lt;377.880005</text>\r\n",
       "</g>\r\n",
       "<!-- 15&#45;&gt;25 -->\r\n",
       "<g id=\"edge33\" class=\"edge\"><title>15&#45;&gt;25</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1210.05,-513.53C1257.77,-522.613 1329.77,-536.32 1381.22,-546.113\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1380.74,-549.585 1391.22,-548.017 1382.05,-542.709 1380.74,-549.585\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-540.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 26 -->\r\n",
       "<g id=\"node35\" class=\"node\"><title>26</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-504\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-500.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.993964851</text>\r\n",
       "</g>\r\n",
       "<!-- 15&#45;&gt;26 -->\r\n",
       "<g id=\"edge34\" class=\"edge\"><title>15&#45;&gt;26</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1219.11,-504C1259.64,-504 1314.03,-504 1359.13,-504\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1359.19,-507.5 1369.19,-504 1359.19,-500.5 1359.19,-507.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-507.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 27 -->\r\n",
       "<g id=\"node38\" class=\"node\"><title>27</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-450\" rx=\"72.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DIS&lt;2.49180007</text>\r\n",
       "</g>\r\n",
       "<!-- 16&#45;&gt;27 -->\r\n",
       "<g id=\"edge37\" class=\"edge\"><title>16&#45;&gt;27</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1205.25,-450C1247.26,-450 1310.81,-450 1361.74,-450\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1361.74,-453.5 1371.74,-450 1361.74,-446.5 1361.74,-453.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-453.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 28 -->\r\n",
       "<g id=\"node39\" class=\"node\"><title>28</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-396\" rx=\"76.8869\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-392.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">AGE&lt;35.8000031</text>\r\n",
       "</g>\r\n",
       "<!-- 16&#45;&gt;28 -->\r\n",
       "<g id=\"edge38\" class=\"edge\"><title>16&#45;&gt;28</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1200.93,-442.206C1246.29,-433.572 1320.18,-419.506 1374.54,-409.158\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1375.29,-412.579 1384.46,-407.271 1373.98,-405.703 1375.29,-412.579\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-432.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 43 -->\r\n",
       "<g id=\"node36\" class=\"node\"><title>43</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-612\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-608.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0456349626</text>\r\n",
       "</g>\r\n",
       "<!-- 25&#45;&gt;43 -->\r\n",
       "<g id=\"edge35\" class=\"edge\"><title>25&#45;&gt;43</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1497.56,-568.157C1541.13,-576.685 1603.77,-588.947 1651.74,-598.335\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1651.25,-601.807 1661.74,-600.293 1652.6,-594.937 1651.25,-601.807\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-594.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 44 -->\r\n",
       "<g id=\"node37\" class=\"node\"><title>44</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-558\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-554.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.292955756</text>\r\n",
       "</g>\r\n",
       "<!-- 25&#45;&gt;44 -->\r\n",
       "<g id=\"edge36\" class=\"edge\"><title>25&#45;&gt;44</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1508.62,-558C1546.67,-558 1595.66,-558 1637.05,-558\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1637.13,-561.5 1647.13,-558 1637.13,-554.5 1637.13,-561.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-561.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 45 -->\r\n",
       "<g id=\"node40\" class=\"node\"><title>45</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-504\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-500.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.174558252</text>\r\n",
       "</g>\r\n",
       "<!-- 27&#45;&gt;45 -->\r\n",
       "<g id=\"edge39\" class=\"edge\"><title>27&#45;&gt;45</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1502.16,-461.058C1546.08,-469.653 1607.15,-481.607 1653.71,-490.72\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1653.22,-494.192 1663.71,-492.678 1654.57,-487.322 1653.22,-494.192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-486.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 46 -->\r\n",
       "<g id=\"node41\" class=\"node\"><title>46</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-450\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-446.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.218155742</text>\r\n",
       "</g>\r\n",
       "<!-- 27&#45;&gt;46 -->\r\n",
       "<g id=\"edge40\" class=\"edge\"><title>27&#45;&gt;46</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1517.79,-450C1553.4,-450 1596.65,-450 1634.13,-450\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1634.53,-453.5 1644.53,-450 1634.53,-446.5 1634.53,-453.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-453.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 47 -->\r\n",
       "<g id=\"node42\" class=\"node\"><title>47</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-396\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-392.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.222304061</text>\r\n",
       "</g>\r\n",
       "<!-- 28&#45;&gt;47 -->\r\n",
       "<g id=\"edge41\" class=\"edge\"><title>28&#45;&gt;47</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1521.42,-396C1557.14,-396 1599.9,-396 1636.71,-396\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1636.93,-399.5 1646.93,-396 1636.93,-392.5 1636.93,-399.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-399.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 48 -->\r\n",
       "<g id=\"node43\" class=\"node\"><title>48</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-342\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-338.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0879577026</text>\r\n",
       "</g>\r\n",
       "<!-- 28&#45;&gt;48 -->\r\n",
       "<g id=\"edge42\" class=\"edge\"><title>28&#45;&gt;48</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1504.18,-384.548C1547.26,-376.115 1606.03,-364.612 1651.6,-355.692\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1652.56,-359.071 1661.7,-353.715 1651.21,-352.201 1652.56,-359.071\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-376.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 29 -->\r\n",
       "<g id=\"node46\" class=\"node\"><title>29</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-234\" rx=\"72.5877\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">DIS&lt;2.21414995</text>\r\n",
       "</g>\r\n",
       "<!-- 17&#45;&gt;29 -->\r\n",
       "<g id=\"edge45\" class=\"edge\"><title>17&#45;&gt;29</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1227.21,-213.399C1269.02,-217.377 1322.9,-222.506 1366.45,-226.651\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1366.14,-230.137 1376.43,-227.601 1366.81,-223.169 1366.14,-230.137\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-226.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 30 -->\r\n",
       "<g id=\"node47\" class=\"node\"><title>30</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-180\" rx=\"90.1842\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-176.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">CRIM&lt;0.0498900004</text>\r\n",
       "</g>\r\n",
       "<!-- 17&#45;&gt;30 -->\r\n",
       "<g id=\"edge46\" class=\"edge\"><title>17&#45;&gt;30</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1227.21,-200.601C1264.67,-197.036 1311.84,-192.547 1352.57,-188.67\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1353.16,-192.131 1362.78,-187.699 1352.49,-185.162 1353.16,-192.131\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-199.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 31 -->\r\n",
       "<g id=\"node52\" class=\"node\"><title>31</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-72\" rx=\"76.8869\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">AGE&lt;83.1000061</text>\r\n",
       "</g>\r\n",
       "<!-- 18&#45;&gt;31 -->\r\n",
       "<g id=\"edge51\" class=\"edge\"><title>18&#45;&gt;31</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1236.01,-72C1273.52,-72 1318.93,-72 1357.7,-72\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1357.93,-75.5001 1367.93,-72 1357.93,-68.5001 1357.93,-75.5001\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-75.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 32 -->\r\n",
       "<g id=\"node53\" class=\"node\"><title>32</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1444.67\" cy=\"-18\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1444.67\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.725492775</text>\r\n",
       "</g>\r\n",
       "<!-- 18&#45;&gt;32 -->\r\n",
       "<g id=\"edge52\" class=\"edge\"><title>18&#45;&gt;32</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1219.11,-60.7456C1264.59,-52.0884 1327.51,-40.1115 1375.18,-31.0374\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1375.94,-34.4557 1385.11,-29.1474 1374.63,-27.5792 1375.94,-34.4557\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1301.83\" y=\"-54.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 49 -->\r\n",
       "<g id=\"node48\" class=\"node\"><title>49</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-288\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-284.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.213525012</text>\r\n",
       "</g>\r\n",
       "<!-- 29&#45;&gt;49 -->\r\n",
       "<g id=\"edge47\" class=\"edge\"><title>29&#45;&gt;49</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1502.16,-245.058C1546.08,-253.653 1607.15,-265.607 1653.71,-274.72\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1653.22,-278.192 1663.71,-276.678 1654.57,-271.322 1653.22,-278.192\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-270.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 50 -->\r\n",
       "<g id=\"node49\" class=\"node\"><title>50</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-234\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.538827837</text>\r\n",
       "</g>\r\n",
       "<!-- 29&#45;&gt;50 -->\r\n",
       "<g id=\"edge48\" class=\"edge\"><title>29&#45;&gt;50</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1517.79,-234C1554.29,-234 1598.81,-234 1636.91,-234\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1636.96,-237.5 1646.96,-234 1636.96,-230.5 1636.96,-237.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-237.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 51 -->\r\n",
       "<g id=\"node50\" class=\"node\"><title>51</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-180\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-176.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0973285735</text>\r\n",
       "</g>\r\n",
       "<!-- 30&#45;&gt;51 -->\r\n",
       "<g id=\"edge49\" class=\"edge\"><title>30&#45;&gt;51</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1535.24,-180C1565.49,-180 1599.29,-180 1629.72,-180\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1629.9,-183.5 1639.9,-180 1629.9,-176.5 1629.9,-183.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-183.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 52 -->\r\n",
       "<g id=\"node51\" class=\"node\"><title>52</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-126\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0071946145</text>\r\n",
       "</g>\r\n",
       "<!-- 30&#45;&gt;52 -->\r\n",
       "<g id=\"edge50\" class=\"edge\"><title>30&#45;&gt;52</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1509.66,-167.476C1551.74,-159.238 1606.87,-148.447 1650.35,-139.938\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1651.14,-143.349 1660.28,-137.993 1649.8,-136.479 1651.14,-143.349\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-160.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 53 -->\r\n",
       "<g id=\"node54\" class=\"node\"><title>53</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-72\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.202670902</text>\r\n",
       "</g>\r\n",
       "<!-- 31&#45;&gt;53 -->\r\n",
       "<g id=\"edge53\" class=\"edge\"><title>31&#45;&gt;53</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1521.42,-72C1557.14,-72 1599.9,-72 1636.71,-72\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1636.93,-75.5001 1646.93,-72 1636.93,-68.5001 1636.93,-75.5001\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-75.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 54 -->\r\n",
       "<g id=\"node55\" class=\"node\"><title>54</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1722.55\" cy=\"-18\" rx=\"73.387\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1722.55\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.04280762</text>\r\n",
       "</g>\r\n",
       "<!-- 31&#45;&gt;54 -->\r\n",
       "<g id=\"edge54\" class=\"edge\"><title>31&#45;&gt;54</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1504.18,-60.548C1548.24,-51.9239 1608.7,-40.0895 1654.67,-31.0908\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1655.41,-34.5139 1664.55,-29.1581 1654.06,-27.6443 1655.41,-34.5139\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1587.51\" y=\"-52.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x20635f54d48>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_graphviz(estimador_xgboost, num_trees=11, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el output de un árbol no está en la misma escala que las predicciones (la variable objetivo tiene el rango 5-50), esto es así por que en el algoritmo XGBoost cada árbol se basa en el output del árbol anterior, intentando corregir el error producido por el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "El algoritmo de stacking simplemente usa el output (generalmente en terminos de probabilidades para casos de clasificacion o de las predicciones en casos de regresión) de múltiples modelos como input para un nuevo metamodelo.\n",
    "\n",
    "scikit learn no tiene un estimador de stacking por defecto, sin embargo, podemos usar el estimador de stacking (StackingRegressor) de mlxtend, una librería que amplia las funcionalidades de sklearn\n",
    "\n",
    "Podemos instalar mlxtend asi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\carlo\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - mlxtend\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    mlxtend-0.18.0             |     pyhd3deb0d_0         1.2 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  mlxtend            conda-forge/noarch::mlxtend-0.18.0-pyhd3deb0d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "mlxtend-0.18.0       | 1.2 MB    |            |   0% \n",
      "mlxtend-0.18.0       | 1.2 MB    | 1          |   1% \n",
      "mlxtend-0.18.0       | 1.2 MB    | #          |  10% \n",
      "mlxtend-0.18.0       | 1.2 MB    | ##5        |  26% \n",
      "mlxtend-0.18.0       | 1.2 MB    | ####1      |  41% \n",
      "mlxtend-0.18.0       | 1.2 MB    | ######9    |  69% \n",
      "mlxtend-0.18.0       | 1.2 MB    | ########3  |  83% \n",
      "mlxtend-0.18.0       | 1.2 MB    | ########## | 100% \n",
      "mlxtend-0.18.0       | 1.2 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Stacking regressor for scikit-learn estimators for regression.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    regressors : array-like, shape = [n_regressors]\n",
      "        A list of regressors.\n",
      "        Invoking the `fit` method on the `StackingRegressor` will fit clones\n",
      "        of those original regressors that will\n",
      "        be stored in the class attribute\n",
      "        `self.regr_`.\n",
      "    meta_regressor : object\n",
      "        The meta-regressor to be fitted on the ensemble of\n",
      "        regressors\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "        - `verbose=0` (default): Prints nothing\n",
      "        - `verbose=1`: Prints the number & name of the regressor being fitted\n",
      "        - `verbose=2`: Prints info about the parameters of the\n",
      "                       regressor being fitted\n",
      "        - `verbose>2`: Changes `verbose` param of the underlying regressor to\n",
      "           self.verbose - 2\n",
      "    use_features_in_secondary : bool (default: False)\n",
      "        If True, the meta-regressor will be trained both on\n",
      "        the predictions of the original regressors and the\n",
      "        original dataset.\n",
      "        If False, the meta-regressor will be trained only on\n",
      "        the predictions of the original regressors.\n",
      "    store_train_meta_features : bool (default: False)\n",
      "        If True, the meta-features computed from the training data\n",
      "        used for fitting the\n",
      "        meta-regressor stored in the `self.train_meta_features_` array,\n",
      "        which can be\n",
      "        accessed after calling `fit`.\n",
      "\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    regr_ : list, shape=[n_regressors]\n",
      "        Fitted regressors (clones of the original regressors)\n",
      "    meta_regr_ : estimator\n",
      "        Fitted meta-regressor (clone of the original meta-estimator)\n",
      "    coef_ : array-like, shape = [n_features]\n",
      "        Model coefficients of the fitted meta-estimator\n",
      "    intercept_ : float\n",
      "        Intercept of the fitted meta-estimator\n",
      "    train_meta_features : numpy array,\n",
      "        shape = [n_samples, len(self.regressors)]\n",
      "        meta-features for training data, where n_samples is the\n",
      "        number of samples\n",
      "        in training data and len(self.regressors) is the number of regressors.\n",
      "    refit : bool (default: True)\n",
      "        Clones the regressors for stacking regression if True (default)\n",
      "        or else uses the original ones, which will be refitted on the dataset\n",
      "        upon calling the `fit` method. Setting refit=False is\n",
      "        recommended if you are working with estimators that are supporting\n",
      "        the scikit-learn fit/predict API interface but are not compatible\n",
      "        to scikit-learn's `clone` function.\n",
      "\n",
      "    Examples\n",
      "    -----------\n",
      "    For usage examples, please see\n",
      "    http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(StackingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.234956303850679"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_stacking = StackingRegressor(\n",
    "    regressors=[\n",
    "        BaggingRegressor(n_estimators=100),\n",
    "        AdaBoostRegressor(n_estimators=100),\n",
    "        GradientBoostingRegressor(n_estimators=100),\n",
    "        RandomForestRegressor(n_estimators=100)\n",
    "    ], \n",
    "    meta_regressor=XGBRegressor(n_estimators=100))\n",
    "\n",
    "\n",
    "error_cv = cross_val_score(estimador_stacking, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"stacking\"] = error_cv\n",
    "\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.930799537642789,\n",
       " 'elasticnet': 5.261057069533588,\n",
       " 'lasso': 5.4644362815333665,\n",
       " 'ridge': 5.095150164969335,\n",
       " 'bagging_arbol_100': 4.464606082289898,\n",
       " 'bagging_elnet': 5.255074676269464,\n",
       " 'bagging_extra_arbol': 3.933019635315263,\n",
       " 'adaboost_100': 4.442665762621013,\n",
       " 'gradientboost_100': 3.8877327620537825,\n",
       " 'randomforest_100': 4.192368736484096,\n",
       " 'xgboost_100': 4.369623838543127,\n",
       " 'stacking': 4.234956303850679}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
